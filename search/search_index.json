{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Gait Recogniton For full code implementation visit github/gait_recogntition . Project layout User Guide - (Installation guide and requirements) OpenGait - (Training the model) Prototype - (GaitSearch) React FastAPI","title":"Home"},{"location":"#gait-recogniton","text":"For full code implementation visit github/gait_recogntition .","title":"Gait Recogniton"},{"location":"#project-layout","text":"User Guide - (Installation guide and requirements) OpenGait - (Training the model) Prototype - (GaitSearch) React FastAPI","title":"Project layout"},{"location":"about/","text":"About Us Team Members Dharmapuri Krishna Sathvik : Github Ngui Jia Xuan, Sheriann : Github Vincent Leonardo : Github Sarah Wong : Github Sarah Ramjoo : Github Project Repo Github","title":"About"},{"location":"about/#about-us","text":"","title":"About Us"},{"location":"about/#team-members","text":"Dharmapuri Krishna Sathvik : Github Ngui Jia Xuan, Sheriann : Github Vincent Leonardo : Github Sarah Wong : Github Sarah Ramjoo : Github","title":"Team Members"},{"location":"about/#project-repo","text":"Github","title":"Project Repo"},{"location":"opengait/opengait/","text":"Model Training Prepare dataset See prepare dataset . Get trained model Option 1: python misc/download_pretrained_model.py Option 2: Go to the release page , then download the model file and uncompress it to output . Train Train a model by CUDA_VISIBLE_DEVICES=0,1 python -m torch.distributed.launch --nproc_per_node=2 opengait/main.py --cfgs ./config/baseline/baseline.yaml --phase train - python -m torch.distributed.launch DDP launch instruction. - --nproc_per_node The number of gpus to use, and it must equal the length of CUDA_VISIBLE_DEVICES . - --cfgs The path to config file. - --phase Specified as train . --log_to_file If specified, the terminal log will be written on disk simultaneously. You can run commands in train.sh for training different models. Training is done on a Linux environment. Commands and code would have to be changed manually according to your specific OS environment. Models We Created To run the models we created, a Gait Ensemble of GaitSet + GaitPart and a Hybrid model, use these commands: Gait Ensemble CUDA_VISIBLE_DEVICES = 0 python -m torch.distributed.launch --nproc_per_node = 2 opengait/main.py --cfgs ./config/gaitens.yaml --phase train Hybrid CUDA_VISIBLE_DEVICES = 0 python -m torch.distributed.launch --nproc_per_node = 2 opengait/main.py --cfgs ./config/hybrid.yaml --phase train Test Evaluate the trained model by CUDA_VISIBLE_DEVICES=0,1 python -m torch.distributed.launch --nproc_per_node=2 opengait/main.py --cfgs ./config/baseline/baseline.yaml --phase test --phase : Specified as test , --iter : Specify a iteration checkpoint Tip : Other arguments are the same as train phase. You can run commands in test.sh for testing different models. Customize Read the detailed config to figure out the usage of needed setting items; See how to create your model ; There are some advanced usages, refer to advanced usages , please. Warning In DDP mode, zombie processes may be generated when the program terminates abnormally. You can use this command sh misc/clean_process.sh to clear them.","title":"OpenGait"},{"location":"opengait/opengait/#model-training","text":"","title":"Model Training"},{"location":"opengait/opengait/#prepare-dataset","text":"See prepare dataset .","title":"Prepare dataset"},{"location":"opengait/opengait/#get-trained-model","text":"Option 1: python misc/download_pretrained_model.py Option 2: Go to the release page , then download the model file and uncompress it to output .","title":"Get trained model"},{"location":"opengait/opengait/#train","text":"Train a model by CUDA_VISIBLE_DEVICES=0,1 python -m torch.distributed.launch --nproc_per_node=2 opengait/main.py --cfgs ./config/baseline/baseline.yaml --phase train - python -m torch.distributed.launch DDP launch instruction. - --nproc_per_node The number of gpus to use, and it must equal the length of CUDA_VISIBLE_DEVICES . - --cfgs The path to config file. - --phase Specified as train . --log_to_file If specified, the terminal log will be written on disk simultaneously. You can run commands in train.sh for training different models. Training is done on a Linux environment. Commands and code would have to be changed manually according to your specific OS environment.","title":"Train"},{"location":"opengait/opengait/#models-we-created","text":"To run the models we created, a Gait Ensemble of GaitSet + GaitPart and a Hybrid model, use these commands: Gait Ensemble CUDA_VISIBLE_DEVICES = 0 python -m torch.distributed.launch --nproc_per_node = 2 opengait/main.py --cfgs ./config/gaitens.yaml --phase train Hybrid CUDA_VISIBLE_DEVICES = 0 python -m torch.distributed.launch --nproc_per_node = 2 opengait/main.py --cfgs ./config/hybrid.yaml --phase train","title":"Models We Created"},{"location":"opengait/opengait/#test","text":"Evaluate the trained model by CUDA_VISIBLE_DEVICES=0,1 python -m torch.distributed.launch --nproc_per_node=2 opengait/main.py --cfgs ./config/baseline/baseline.yaml --phase test --phase : Specified as test , --iter : Specify a iteration checkpoint Tip : Other arguments are the same as train phase. You can run commands in test.sh for testing different models.","title":"Test"},{"location":"opengait/opengait/#customize","text":"Read the detailed config to figure out the usage of needed setting items; See how to create your model ; There are some advanced usages, refer to advanced usages , please.","title":"Customize"},{"location":"opengait/opengait/#warning","text":"In DDP mode, zombie processes may be generated when the program terminates abnormally. You can use this command sh misc/clean_process.sh to clear them.","title":"Warning"},{"location":"prototype/fastapi/","text":"Backend Server The prototype requires 2 servers to be run at the same time. The backend server is run on Uvicorn and it uses FastAPI to run the model for prediction. How To Run clone this repository git clone https://github.com/KLASS-gait-recognitionn/GaitSearch change directory to the backend folder cd backend Install relevant dependencies pip install -r requirements.txt run the server uvicorn main:app Endpoints Special requests preflight_handler handles preflight requests sent by the browser before the actual request. The preflight gives the server a chance to examine what the actual request will look like before it's made. @app . options ( '/{rest_of_path:path}' ) async def preflight_handler add_CORS_header enables CORS (Cross-Origin Resource Sharing) to occur between the backend server and the frontend server in the browser @app . middleware ( \"http\" ) async def add_CORS_header ( request : Request , call_next ) POST request upload endpoint posts a file added through the React server to the Backend server. File is converted to .mp4 regardless of filetype. @app . post ( \"/upload\" , response_description = \"\" , response_model = \"\" ) async def upload ( file : UploadFile = File ( ... )): GET requests loading endpoint runs the silhouette extractor in the background when this endpoint is called. The silhoeutte extraction occurs on the video that was uploaded. At the same time, the video is split into a set of images to feed into the model. @app . get ( \"/loading\" ) async def loading (): video endpoint streams the original video with a bounding box attached to the subject to focus on. @app . get ( \"/video\" ) async def video_endpoint ( range : str = Header ( None )): extract endpoint streams the silhouete extracted video with no bounding box to show that the subject has been properly extracted. @app . get ( \"/extract\" ) async def extractor_endpoint ( range : str = Header ( None )): opengait endpoint runs the model on the background on the set of images to create an embedding vector of the subject in the video. The embedding vector is compared with other embedding vectors in the database using the smallest Euclidean distance to give a subject-id of another video that matches with the embedding vector of the original video. @app . get ( \"/opengait\" ) async def opengait_embeddings (): embed1 endpoint streams the original video @app . get ( \"/embed1\" , response_class = StreamingResponse ) async def video_endpoint ( range : str = Header ( None )): embed2 endpoint streams the video that has the most similar embedding vector to the subject in the original video @app . get ( \"/embed2\" , response_class = StreamingResponse ) async def video_endpoint ( range : str = Header ( None )):","title":"FastAPI"},{"location":"prototype/fastapi/#backend-server","text":"The prototype requires 2 servers to be run at the same time. The backend server is run on Uvicorn and it uses FastAPI to run the model for prediction.","title":"Backend Server"},{"location":"prototype/fastapi/#how-to-run","text":"clone this repository git clone https://github.com/KLASS-gait-recognitionn/GaitSearch change directory to the backend folder cd backend Install relevant dependencies pip install -r requirements.txt run the server uvicorn main:app","title":"How To Run"},{"location":"prototype/fastapi/#endpoints","text":"","title":"Endpoints"},{"location":"prototype/fastapi/#special-requests","text":"preflight_handler handles preflight requests sent by the browser before the actual request. The preflight gives the server a chance to examine what the actual request will look like before it's made. @app . options ( '/{rest_of_path:path}' ) async def preflight_handler add_CORS_header enables CORS (Cross-Origin Resource Sharing) to occur between the backend server and the frontend server in the browser @app . middleware ( \"http\" ) async def add_CORS_header ( request : Request , call_next )","title":"Special requests"},{"location":"prototype/fastapi/#post-request","text":"upload endpoint posts a file added through the React server to the Backend server. File is converted to .mp4 regardless of filetype. @app . post ( \"/upload\" , response_description = \"\" , response_model = \"\" ) async def upload ( file : UploadFile = File ( ... )):","title":"POST request"},{"location":"prototype/fastapi/#get-requests","text":"loading endpoint runs the silhouette extractor in the background when this endpoint is called. The silhoeutte extraction occurs on the video that was uploaded. At the same time, the video is split into a set of images to feed into the model. @app . get ( \"/loading\" ) async def loading (): video endpoint streams the original video with a bounding box attached to the subject to focus on. @app . get ( \"/video\" ) async def video_endpoint ( range : str = Header ( None )): extract endpoint streams the silhouete extracted video with no bounding box to show that the subject has been properly extracted. @app . get ( \"/extract\" ) async def extractor_endpoint ( range : str = Header ( None )): opengait endpoint runs the model on the background on the set of images to create an embedding vector of the subject in the video. The embedding vector is compared with other embedding vectors in the database using the smallest Euclidean distance to give a subject-id of another video that matches with the embedding vector of the original video. @app . get ( \"/opengait\" ) async def opengait_embeddings (): embed1 endpoint streams the original video @app . get ( \"/embed1\" , response_class = StreamingResponse ) async def video_endpoint ( range : str = Header ( None )): embed2 endpoint streams the video that has the most similar embedding vector to the subject in the original video @app . get ( \"/embed2\" , response_class = StreamingResponse ) async def video_endpoint ( range : str = Header ( None )):","title":"GET requests"},{"location":"prototype/react/","text":"Frontend Server The prototype requires 2 servers to be run at the same time. The frontend server uses a Node. js server to prerender React components. How To Run clone this repository git clone https://github.com/KLASS-gait-recognitionn/GaitSearch change directory to the frontend folder cd react Install relevant dependencies npm install run the server npm start Open http://localhost:3000 in your browser","title":"React"},{"location":"prototype/react/#frontend-server","text":"The prototype requires 2 servers to be run at the same time. The frontend server uses a Node. js server to prerender React components.","title":"Frontend Server"},{"location":"prototype/react/#how-to-run","text":"clone this repository git clone https://github.com/KLASS-gait-recognitionn/GaitSearch change directory to the frontend folder cd react Install relevant dependencies npm install run the server npm start Open http://localhost:3000 in your browser","title":"How To Run"},{"location":"prototype/react/#_1","text":"","title":""},{"location":"user-guide/getting-started/","text":"User Guide This is a user guide to install the entire project. Getting Started OpenGait In github/gait_training , we added an OpenGait folder that is pulled from another repo. We trained our models in this repo. For the latest updated code, please visit and clone the official OpenGait repo instead. Installation clone this repo. git clone https://github.com/KLASS-gait-recognition/gait_training Install dependencies: pytorch >= 1.6 torchvision pyyaml tensorboard opencv-python tqdm py7zr Install dependencies by Anaconda : conda install tqdm pyyaml tensorboard opencv py7zr conda install pytorch==1.6.0 torchvision -c pytorch Or, Install dependencies by pip: pip install tqdm pyyaml tensorboard opencv-python py7zr pip install torch==1.6.0 torchvision==0.7.0 Nota bene: due to certain conflicts in dependencies, installing an older version of setuptools might be required. pip install setuptools==59.5.0 React To run the prototype install node.js server. Installation clone this repo. git clone https://github.com/KLASS-gait-recognition/GaitSearch Install node.js and npm You\u2019ll need to have Node 14.0.0 or later version on your local development machine (but it\u2019s not required on the server). We recommend using the latest LTS version. You can use nvm (macOS/Linux) or nvm-windows to switch Node versions between different projects. For more information visit the official website of node.js Install dependencies: @chakra-ui/react >= 2.2.3, @emotion/react >= 11.9.3, @emotion/styled >= 11.9.3, @material-ui/core >= 4.12.4, @mui/material > = 5.9.0, antd >= 4.21.6, axios >= 0.27.2, firebase >= 9.9.0, framer-motion >= 6.4.3, react >= 18.2.0, react-awesome-button >= 6.5.1, react-bootstrap-validation >= 0.1.11, react-dom >= 18.2.0, react-player >= 2.10.1, react-router-dom >= 6.3.0, react-scripts >= 2.1.3, react-webcam >= 7.0.1, reactstrap >= 9.1.1, styled-components >= 5.3.5, web-vitals >= 2.1.4 Install dependencies by npm : npm install [< package-spec > ...] aliases : add , i , in , ins , inst , insta , instal , isnt , isnta , isntal , isntall Or, to automatically install based on package.json , npm install FastAPI To run the model in the prototype, install FastAPI and uvicorn server. More information at FastAPI official website. Install OpenCV and MMDetection to run the models in the background. Installation Install dependencies: FastAPI Uvicorn OpenCV Install dependencies by Anaconda : conda install -c conda-forge fastapi uvicorn Or, Install dependencies by pip: pip install fastapi \"uvicorn[standard]\" MMdetection Install MMdetection We recommend that users follow our best practices to install MMDetection. However, the whole process is highly customizable. See MMdetection under Customize Installation section for more information. Installation Step 0. Install MMCV using MIM . pip install -U openmim mim install mmcv-full Step 1. Install MMDetection. Case a: If you develop and run mmdet directly, install it from source: git clone https://github.com/open-mmlab/mmdetection.git cd mmdetection pip install -v -e . # \"-v\" means verbose, or more output # \"-e\" means installing a project in editable mode, # thus any local modifications made to the code will take effect without reinstallation. Case B: If you use mmdet as a dependency or third-party package, install it with pip: pip install mmdet This is the method we used. Verify the installation To verify whether MMDetection is installed correctly, we provide some sample codes to run an inference demo. Step 1. We need to download config and checkpoint files. mim download mmdet --config yolov3_mobilenetv2_320_300e_coco --dest . The downloading will take several seconds or more, depending on your network environment. When it is done, you will find two files yolov3_mobilenetv2_320_300e_coco.py and yolov3_mobilenetv2_320_300e_coco_20210719_215349-d18dff72.pth in your current folder. Step 2. Verify the inference demo. Option (a). If you install mmdetection from source, just run the following command. python demo/image_demo.py demo/demo.jpg yolov3_mobilenetv2_320_300e_coco.py yolov3_mobilenetv2_320_300e_coco_20210719_215349-d18dff72.pth --device cpu --out-file result.jpg You will see a new image result.jpg on your current folder, where bounding boxes are plotted on cars, benches, etc. Option (b). If you install mmdetection with pip, open you python interpreter and copy&paste the following codes. from mmdet.apis import init_detector , inference_detector config_file = 'yolov3_mobilenetv2_320_300e_coco.py' checkpoint_file = 'yolov3_mobilenetv2_320_300e_coco_20210719_215349-d18dff72.pth' model = init_detector ( config_file , checkpoint_file , device = 'cpu' ) # or device='cuda:0' inference_detector ( model , 'demo/cat.jpg' ) Since we used pip to install mmdetection, we continued to use option (b). After doing so, you will see a list of arrays printed, indicating the detected bounding boxes. Step 3 [Optional]. Download the SCNet model that is used for silhouette extraction and bounding box detection in the prototype. In normal circumstances, this will be automatically downloaded on first run . mim download mmdet --config scnet_r50_fpn_1x_coco --dest . The downloading will take several seconds or more, depending on your network environment. When it is done, you will find two files scnet_r50_fpn_1x_coco.py and scnet_r50_fpn_1x_coco-c3f09857.pth in your current folder. Place them in the backend folder inside the cloned prototype repo as such configs/scnet/scnet_r50_fpn_1x_coco.py & checkpoints/scnet_r50_fpn_1x_coco-c3f09857.pth Alternatively, you are able to obtain the weights to SCNet-R50-FPN-1x directly here . Simply download the weights and add them to your created backend folder such as checkpoints/scnet_r50_fpn_1x_coco-c3f09857.pth","title":"Getting Started"},{"location":"user-guide/getting-started/#user-guide","text":"This is a user guide to install the entire project.","title":"User Guide"},{"location":"user-guide/getting-started/#getting-started","text":"","title":"Getting Started"},{"location":"user-guide/getting-started/#opengait","text":"In github/gait_training , we added an OpenGait folder that is pulled from another repo. We trained our models in this repo. For the latest updated code, please visit and clone the official OpenGait repo instead.","title":"OpenGait"},{"location":"user-guide/getting-started/#installation","text":"clone this repo. git clone https://github.com/KLASS-gait-recognition/gait_training Install dependencies: pytorch >= 1.6 torchvision pyyaml tensorboard opencv-python tqdm py7zr Install dependencies by Anaconda : conda install tqdm pyyaml tensorboard opencv py7zr conda install pytorch==1.6.0 torchvision -c pytorch Or, Install dependencies by pip: pip install tqdm pyyaml tensorboard opencv-python py7zr pip install torch==1.6.0 torchvision==0.7.0 Nota bene: due to certain conflicts in dependencies, installing an older version of setuptools might be required. pip install setuptools==59.5.0","title":"Installation"},{"location":"user-guide/getting-started/#react","text":"To run the prototype install node.js server.","title":"React"},{"location":"user-guide/getting-started/#installation_1","text":"clone this repo. git clone https://github.com/KLASS-gait-recognition/GaitSearch Install node.js and npm You\u2019ll need to have Node 14.0.0 or later version on your local development machine (but it\u2019s not required on the server). We recommend using the latest LTS version. You can use nvm (macOS/Linux) or nvm-windows to switch Node versions between different projects. For more information visit the official website of node.js Install dependencies: @chakra-ui/react >= 2.2.3, @emotion/react >= 11.9.3, @emotion/styled >= 11.9.3, @material-ui/core >= 4.12.4, @mui/material > = 5.9.0, antd >= 4.21.6, axios >= 0.27.2, firebase >= 9.9.0, framer-motion >= 6.4.3, react >= 18.2.0, react-awesome-button >= 6.5.1, react-bootstrap-validation >= 0.1.11, react-dom >= 18.2.0, react-player >= 2.10.1, react-router-dom >= 6.3.0, react-scripts >= 2.1.3, react-webcam >= 7.0.1, reactstrap >= 9.1.1, styled-components >= 5.3.5, web-vitals >= 2.1.4 Install dependencies by npm : npm install [< package-spec > ...] aliases : add , i , in , ins , inst , insta , instal , isnt , isnta , isntal , isntall Or, to automatically install based on package.json , npm install","title":"Installation"},{"location":"user-guide/getting-started/#fastapi","text":"To run the model in the prototype, install FastAPI and uvicorn server. More information at FastAPI official website. Install OpenCV and MMDetection to run the models in the background.","title":"FastAPI"},{"location":"user-guide/getting-started/#installation_2","text":"Install dependencies: FastAPI Uvicorn OpenCV Install dependencies by Anaconda : conda install -c conda-forge fastapi uvicorn Or, Install dependencies by pip: pip install fastapi \"uvicorn[standard]\"","title":"Installation"},{"location":"user-guide/getting-started/#mmdetection","text":"Install MMdetection We recommend that users follow our best practices to install MMDetection. However, the whole process is highly customizable. See MMdetection under Customize Installation section for more information.","title":"MMdetection"},{"location":"user-guide/getting-started/#installation_3","text":"Step 0. Install MMCV using MIM . pip install -U openmim mim install mmcv-full Step 1. Install MMDetection. Case a: If you develop and run mmdet directly, install it from source: git clone https://github.com/open-mmlab/mmdetection.git cd mmdetection pip install -v -e . # \"-v\" means verbose, or more output # \"-e\" means installing a project in editable mode, # thus any local modifications made to the code will take effect without reinstallation. Case B: If you use mmdet as a dependency or third-party package, install it with pip: pip install mmdet This is the method we used.","title":"Installation"},{"location":"user-guide/getting-started/#verify-the-installation","text":"To verify whether MMDetection is installed correctly, we provide some sample codes to run an inference demo. Step 1. We need to download config and checkpoint files. mim download mmdet --config yolov3_mobilenetv2_320_300e_coco --dest . The downloading will take several seconds or more, depending on your network environment. When it is done, you will find two files yolov3_mobilenetv2_320_300e_coco.py and yolov3_mobilenetv2_320_300e_coco_20210719_215349-d18dff72.pth in your current folder. Step 2. Verify the inference demo. Option (a). If you install mmdetection from source, just run the following command. python demo/image_demo.py demo/demo.jpg yolov3_mobilenetv2_320_300e_coco.py yolov3_mobilenetv2_320_300e_coco_20210719_215349-d18dff72.pth --device cpu --out-file result.jpg You will see a new image result.jpg on your current folder, where bounding boxes are plotted on cars, benches, etc. Option (b). If you install mmdetection with pip, open you python interpreter and copy&paste the following codes. from mmdet.apis import init_detector , inference_detector config_file = 'yolov3_mobilenetv2_320_300e_coco.py' checkpoint_file = 'yolov3_mobilenetv2_320_300e_coco_20210719_215349-d18dff72.pth' model = init_detector ( config_file , checkpoint_file , device = 'cpu' ) # or device='cuda:0' inference_detector ( model , 'demo/cat.jpg' ) Since we used pip to install mmdetection, we continued to use option (b). After doing so, you will see a list of arrays printed, indicating the detected bounding boxes. Step 3 [Optional]. Download the SCNet model that is used for silhouette extraction and bounding box detection in the prototype. In normal circumstances, this will be automatically downloaded on first run . mim download mmdet --config scnet_r50_fpn_1x_coco --dest . The downloading will take several seconds or more, depending on your network environment. When it is done, you will find two files scnet_r50_fpn_1x_coco.py and scnet_r50_fpn_1x_coco-c3f09857.pth in your current folder. Place them in the backend folder inside the cloned prototype repo as such configs/scnet/scnet_r50_fpn_1x_coco.py & checkpoints/scnet_r50_fpn_1x_coco-c3f09857.pth Alternatively, you are able to obtain the weights to SCNet-R50-FPN-1x directly here . Simply download the weights and add them to your created backend folder such as checkpoints/scnet_r50_fpn_1x_coco-c3f09857.pth","title":"Verify the installation"}]}